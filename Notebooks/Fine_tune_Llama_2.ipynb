{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMe8dNNW4H/PQCOEgEQ4IPN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing all required Libraries"
      ],
      "metadata": {
        "id": "4TbR-GdSwYPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FgAtI_WbtjI5",
        "outputId": "28e8060d-b21f-4f47-abee-520212701fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.21.0\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.3.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2024.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate==0.21.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft==0.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Yd_TGWqUwjo3",
        "outputId": "12f3e67d-367b-4176-fb8d-414de3389283"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft==0.4.0\n",
            "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/72.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/72.9 kB\u001b[0m \u001b[31m785.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m71.7/72.9 kB\u001b[0m \u001b[31m998.8 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m853.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (2.3.0+cpu)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (4.41.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (0.21.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (2024.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0) (0.23.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\n",
            "Installing collected packages: peft\n",
            "Successfully installed peft-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes==0.40.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-rW-eOMYvBow",
        "outputId": "0b2903f9-3131-41be-85f2-06fb4b8c3a66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes==0.40.2\n",
            "  Downloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.40.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.31.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lvmo8GOxwu0n",
        "outputId": "eb765665-aa48-46fd-bf3a-b6a682dfd75e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.31.0\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2024.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2024.6.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl==0.4.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h4RVZk5CwzIa",
        "outputId": "1a661afc-3176-4a3c-93f7-98fdce711b28"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl==0.4.7\n",
            "  Downloading trl-0.4.7-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m917.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.4.7) (2.3.0+cpu)\n",
            "Requirement already satisfied: transformers>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.4.7) (4.31.0)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl==0.4.7) (1.25.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl==0.4.7) (0.21.0)\n",
            "Collecting datasets (from trl==0.4.7)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.7) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.7) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.7) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.7) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.7) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.7) (2024.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.7) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.7) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.7) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.7) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.7) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.7) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.7) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.7) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl==0.4.7) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.7) (16.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets->trl==0.4.7)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->trl==0.4.7)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.7) (2.0.3)\n",
            "Collecting requests (from transformers>=4.18.0->trl==0.4.7)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets->trl==0.4.7)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->trl==0.4.7)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec[http]<=2024.5.0,>=2023.1.0 (from datasets->trl==0.4.7)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp (from datasets->trl==0.4.7)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->trl==0.4.7)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.7) (23.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->trl==0.4.7)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->trl==0.4.7)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->trl==0.4.7)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->trl==0.4.7)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.7) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.7) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.7) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.7) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl==0.4.7) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.4.7) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.4.7) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.4.7) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl==0.4.7) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.4.7) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow-hotfix, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets, trl\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.0\n",
            "    Uninstalling fsspec-2024.6.0:\n",
            "      Successfully uninstalled fsspec-2024.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-hotfix-0.6 requests-2.32.3 trl-0.4.7 xxhash-3.4.1 yarl-1.9.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Required Libraries"
      ],
      "metadata": {
        "id": "dcCyETODxAWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "Hz0AXNa3xEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case of Llama 2, the following prompt template is used for the chat models:\n",
        "\n",
        "**System Prompt** (optional) to guide the model\n",
        "\n",
        "**User prompt** (required) to give the instruction\n",
        "\n",
        "**Model Answer** (required)\n",
        "\n",
        "\n",
        "**System prompts** are used to provide context or additional information that guides the model's response. They can vary widely based on the application or platform.\n",
        "\n",
        "Example System Prompt: \"You are chatting with a virtual assistant trained to answer questions about technology. Please ask a question related to software development.\"\n",
        "\n",
        "In this example, the system prompt sets the context that the user is interacting with a virtual assistant focused on technology-related questions.\n",
        "\n",
        "**User prompts** are essential as they instruct the model on what information or response is expected. They frame the context for the model's generation.\n",
        "\n",
        "Example User Prompt: \"How can I optimize my code for performance in Python?\"\n",
        "\n",
        "**Model Answer (Generated):**\n",
        "The model generates a response based on the provided prompts. This response should ideally address the user's query or instruction.\n",
        "\n",
        "Example Model Answer: \"To optimize your Python code for performance, consider using techniques such as optimizing loops with list comprehensions, using built-in functions like map() and filter(), and leveraging libraries like NumPy for numerical computations. Additionally, profiling your code with tools like cProfile can help identify bottlenecks and optimize accordingly.\"\n",
        "\n",
        "\n",
        "**Conclusion:**\n",
        "The prompt structure of System Prompt (optional), User Prompt (required), and Model Answer (required) provides a clear framework for interacting with pre-trained language models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bwU3ow0Xy2uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "WdTFmPbw3VPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Model"
      ],
      "metadata": {
        "id": "XLUU_j2w86FR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load a llama-2-7b-chat-hf model (chat model)\n",
        "2. Train it on the Amod/mental_health_counseling_conversations, which will produce our fine-tuned model Llama-2-7b-chat-finetune\n",
        "\n",
        "QLoRA will use a rank of 64 with a scaling parameter of 16. We’ll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for one epoch"
      ],
      "metadata": {
        "id": "qMhp-lru8_yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"Amod/mental_health_counseling_conversations\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"Llama-2-7b-chat-finetune\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "ThYaqSTT3U65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning"
      ],
      "metadata": {
        "id": "1R0oUjKb-sG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.\n",
        "\n",
        "2. Then, we’re configuring bitsandbytes for 4-bit quantization.\n",
        "\n",
        "3. Next, we're loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\n",
        "\n",
        "4. Finally, we're loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!\n",
        "\n"
      ],
      "metadata": {
        "id": "aukv-oYC-ixu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (you can process it here)\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Custom prompt template for Llama 2\n",
        "def create_prompt(context, response):\n",
        "    return f\"[INST] {context} [/INST] {response}\"\n",
        "\n",
        "# Preprocess dataset to match the prompt template\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"Context\"]\n",
        "    targets = examples[\"Response\"]\n",
        "    return {\"text\": [create_prompt(inp, tgt) for inp, tgt in zip(inputs, targets)]}\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        ""
      ],
      "metadata": {
        "id": "pe3kJnz_y2Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "-jEZfhbNxEf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the plots on tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BAZOnMXmCPag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation pipeline with the new model\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Example context (user input)\n",
        "context = \"I can't seem to feel any emotion except anxiety, not even for myself.\"\n",
        "\n",
        "# Generate response\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"[INST] {context} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "R8OFYU8NCZxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "ASXHviA7Tiil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "kRjtZWb3UGr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device map for loading the model\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# Reload the base model in FP16 precision\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "\n",
        "# Merge the base model with the fine-tuned LoRA weights\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save the merged model\n",
        "merged_model_dir = \"Llama-2-7b-chat-merged\"\n",
        "model.save_pretrained(merged_model_dir)\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.save_pretrained(merged_model_dir)"
      ],
      "metadata": {
        "id": "BEnnM3crUhXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the merged model and tokenizer\n",
        "merged_model_dir = \"Llama-2-7b-chat-merged\"\n",
        "model = AutoModelForCausalLM.from_pretrained(merged_model_dir, torch_dtype=torch.float16).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(merged_model_dir)\n",
        "\n",
        "# Text generation pipeline with the merged model\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "\n",
        "def generate_response(prompt, max_length=200):\n",
        "    response = pipe(prompt, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
        "    return response[0]['generated_text']\n",
        "\n",
        "def format_prompt(context):\n",
        "    return f\"[INST] {context} [/INST]\"\n",
        "\n",
        "# Chatbot conversation loop\n",
        "print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    prompt = format_prompt(user_input)\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Chatbot: {response}\")\n",
        "\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "id": "8NdEopJHU8cg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}