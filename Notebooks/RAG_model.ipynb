{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "google_key=os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=google_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Storing the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge multiple PDFs into one\n",
    "def merge_pdfs_from_folder(folder_path, output_pdf_path):\n",
    "    \"\"\"\n",
    "    Merges all PDFs in the specified folder into a single PDF.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing PDFs.\n",
    "        output_pdf_path (str): Path to save the merged PDF.\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the merged PDF.\n",
    "    \"\"\"\n",
    "    # Create a new empty PDF document\n",
    "    output_pdf = fitz.open()\n",
    "\n",
    "    # Loop through all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".pdf\"):  # Only consider PDF files\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            input_pdf = fitz.open(file_path)  # Open the PDF\n",
    "            output_pdf.insert_pdf(input_pdf)  # Insert pages into the output PDF\n",
    "            input_pdf.close()  # Close the input PDF after merging\n",
    "\n",
    "    # Save the merged PDF\n",
    "    output_pdf.save(output_pdf_path)\n",
    "    output_pdf.close()\n",
    "\n",
    "    return output_pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pdf=merge_pdfs_from_folder(\"Empath-AI/Data/Mental_Health-PDFS\",\"Empath-AI/Data/Final.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text=extract_text_from_pdf(\"Empath-AI/Data/Final.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_text_into_chunks(text):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_chunks=split_text_into_chunks(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: MENTAL \n",
      "HEALTH CARE\n",
      " in Settings Where Mental Health \n",
      "Resources Are Limited\n",
      "An Easy-Reference \n",
      "GUIDEBOOK \n",
      "for Healthcare Providers\n",
      " in Developed and Developing Countries\n",
      "PA M E L A  S M I T H ,  M D\n",
      " \n",
      "Chunk 1: An Easy-Reference Guidebook for Healthcare Providers \n",
      "in Developed and Developing Countries\n",
      " Copyright © 2014 Pamela Smith.\n",
      "All rights reserved. No part of this book may be used or reproduced by any m\n",
      "Chunk 2: Archway Publishing books may be ordered through booksellers or by contacting:\n",
      "Archway Publishing\n",
      "1663 Liberty Drive\n",
      "Bloomington, IN 47403\n",
      "www.archwaypublishing.com\n",
      "1-(888)-242-5904\n",
      "Because of the dyna\n",
      "Chunk 3: views of the publisher, and the publisher hereby disclaims any responsibility for them.\n",
      "The \u0003 eld guide is not a substitute for comprehensive psychiatry, psychology, or \n",
      "other related mental health te\n",
      "Chunk 4: describe generally accepted practices. Application of this information in a particular \n",
      "situation remains the responsibility of the practitioner or health care provider.\n",
      "Certain stock imagery © Thinks\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(extracted_chunks[:5]):\n",
    "    print(f\"Chunk {i}: {chunk[:200]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_embeddings(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.strip():  # Ensure the chunk is not empty\n",
    "            response = genai.embed_content(\n",
    "                model=\"models/text-embedding-004\",  # Gemini Pro embedding model\n",
    "                content=chunk\n",
    "            )\n",
    "            \n",
    "            # Now we directly access 'embedding' as it contains the values directly\n",
    "            if isinstance(response, dict) and 'embedding' in response:\n",
    "                embeddings.append(response['embedding'])  # Append the embedding directly\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data=generate_gemini_embeddings(extracted_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pinecone_key=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gupta\\Documents\\GitHub\\Empath-AI\\empath_env\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "pc=Pinecone(api_key=pinecone_key)\n",
    "index=pc.Index(\"empath-ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_embeddings_in_batches(text_chunks, embeddings, batch_size=100):\n",
    "    vectors = []\n",
    "    \n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        # Create metadata for each chunk\n",
    "        metadata = {\"text\": text_chunks[i], \"source\": \"your_document_source\"}\n",
    "        vector = {\n",
    "            \"id\": f\"vec{i}\",  # Unique ID for each vector\n",
    "            \"values\": embedding,  # The embedding values\n",
    "            \"metadata\": metadata  # Metadata for the chunk\n",
    "        }\n",
    "        vectors.append(vector)\n",
    "        \n",
    "        # Batch upsert every `batch_size` chunks\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(embeddings):\n",
    "            index.upsert(vectors=vectors, namespace=\"ns1\")\n",
    "            vectors = []  # Clear the list after each batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to upsert embeddings in batches\n",
    "upsert_embeddings_in_batches(extracted_chunks, embedded_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_embedding(query):\n",
    "    response = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",  # Gemini Pro embedding model\n",
    "        content=query\n",
    "    )\n",
    "    \n",
    "    # Extract and return the embedding from the response\n",
    "    if 'embedding' in response:\n",
    "        return response['embedding']\n",
    "    else:\n",
    "        raise ValueError(f\"Failed to generate embeddings for query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query_embedding):\n",
    "    # Search Pinecone index using the query embedding\n",
    "    query_response = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=2,  # Adjust the number of top results you want\n",
    "        include_metadata=True,  # Return metadata (e.g., source information) along with vectors\n",
    "        namespace=\"ns1\" \n",
    "    )\n",
    "    \n",
    "    # Extract relevant chunks from the Pinecone response\n",
    "    retrieved_chunks = [match['metadata']['text'] for match in query_response['matches']]\n",
    "\n",
    "    \n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def query_and_retrieve(query):\n",
    "#     # Step 1: Generate the embedding for the query\n",
    "#     query_embedding = generate_query_embedding(query)\n",
    "    \n",
    "#     # Step 2: Retrieve relevant chunks using the generated embedding\n",
    "#     retrieved_texts = retrieve_relevant_chunks(query_embedding)\n",
    "    \n",
    "#     # Step 3: Return the retrieved texts\n",
    "#     return retrieved_texts\n",
    "\n",
    "# # Example Usage\n",
    "# query = \"What are the symptoms of anxiety?\"\n",
    "# retrieved_texts = query_and_retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to use Gemini Pro LLM to generate the final answer\n",
    "def generate_answer_with_gemini(query, retrieved_chunks):\n",
    "    # Combine the retrieved chunks into a single context\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    # Crafting the prompt for GODAI\n",
    "    prompt = f\"\"\"\n",
    "    You are EMPATH-AI, created by Soham and Priya, designed to offer personalized emotional support based on a detailed assessment \n",
    "    of the user's emotional and mental health state. \n",
    "    Your tone should feel like that of a caring and close friend—warm, understanding, and encouraging. Be specific and personal when providing advice, referencing practical steps the user can take. \n",
    "\n",
    "    If suicidal thoughts are detected, prioritize providing immediate comfort and specific, actionable steps. This might include grounding techniques like deep breathing or asking the user to reach out to someone they trust. \n",
    "\n",
    "    When referring to coping strategies, include specific suggestions. For example:\n",
    "    - \"Studies show that deep breathing exercises for 5 minutes a day can significantly reduce anxiety. How about giving it a try? Here’s a simple breathing technique you can follow: breathe in for 4 seconds, hold for 4, and breathe out for 4.\"\n",
    "    - \"Experts recommend creating small goals, like getting up at the same time each day or setting a bedtime, to help improve mood and reduce anxiety. You could try this by setting a 10 PM bedtime as a start.\"\n",
    "\n",
    "    When relevant, refer to **specific research or studies**. For example:\n",
    "    - \"According to a study published in the Journal of Clinical Psychology, practicing gratitude for just 5 minutes a day can help improve mood. Maybe try writing down three things you’re grateful for each morning.\"\n",
    "    - \"Research shows that regular exercise, even just 20 minutes of walking, has been linked to reducing feelings of anxiety and depression. How about we set a small goal for today—maybe a quick walk around the block?\"\n",
    "\n",
    "    Always provide easy-to-implement actions, and make sure your suggestions feel supportive and conversational. Your goal is to provide practical steps they can easily follow, while making them feel understood and supported.\n",
    "\n",
    "    \n",
    "\n",
    "    The following is the input that includes the detailed emotional assessment and explanations provided by the first model:\n",
    "    \"{query}\"\n",
    "\n",
    "    Now, based on the retrieved information from some of the psychological books provided below and the user's current emotional state, offer a friendly, specific, and supportive response. Include research-backed suggestions where appropriate and provide actionable steps the user can take to feel better:\n",
    "    RETRIEVED INFORMATION: {context}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    response = model.generate_content([prompt])\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrived Chunks: ['stomach. I wasn’t sick — I didn’t have a fever or something wrong. I don’t\\nknow if it’s related, but my boyfriend has this girl, this girlfriend that he\\ngoes down to see, and this particular weekend I didn’t want him to go\\ndown — I hadn’t seen him for a long time because of the vacation. What’s\\nreally disgusting is that next weekend she’s coming up here! That about\\nkicked me over the edge. Could it be that the pains in my stomach come\\nfrom that, that I’m more afraid of losing him than I think?”', 'a very small step; the “You don’t show it” and “You show it to that baby” are\\ncombined.]\\nClient: You show it to that baby and not to me! You show it to that baby and not to\\nme!!! [increasing anger] IT’S ALL GONE!!\\nTherapist: Pound the pillow! [a direction] Say, “ You took your love away and gave it\\nto that baby! Say that!; [This prompt amplified the feeling “It’s all gone” and\\n“You show it to that baby.”]\\nClient: You took your love away and gave it to that baby. You took your love away']\n",
      "\n",
      "EMPATH-AI's Answer:\n",
      "My heart goes out to you, hearing about your boyfriend's hurtful actions. It's understandable that you're feeling devastated and angry. I want to assure you that you're not alone and there are people who care about you and want to help.\n",
      "\n",
      "Violence or harm is never the answer. If you're feeling like you might hurt yourself or others, it's crucial to seek immediate help. Remember that there is hope and support available. You can call the National Suicide Prevention Lifeline at 988 or reach out to a trusted friend or family member.\n",
      "\n",
      "It's also important to prioritize your well-being during this challenging time. Here are a few actionable steps you might consider:\n",
      "\n",
      "* **Practice self-care:** Engage in activities that nourish your mental and physical health, such as spending time in nature, exercising, or connecting with loved ones.\n",
      "* **Seek professional support:** Consider reaching out to a therapist or counselor who can provide a safe space to process your emotions and develop coping strategies.\n",
      "* **Practice mindfulness:** Pay attention to your thoughts and feelings without judgment. This can help you gain a better understanding of your emotions and identify triggers.\n",
      "* **Set boundaries:** Communicate your needs and expectations clearly to your boyfriend. Let him know that his actions are unacceptable and that you will not tolerate disrespect.\n",
      "* **Focus on self-love:** Remember your worth and value. Treat yourself with kindness and compassion, especially during difficult times.\n",
      "\n",
      "Remember, healing takes time and effort. Be patient with yourself and don't give up on your journey towards well-being. If you need someone to talk to or want additional support, please don't hesitate to reach out. I'm here for you every step of the way.\n"
     ]
    }
   ],
   "source": [
    "# Main function to handle user input and process the query through GODAI\n",
    "def main():\n",
    "    # Prompt user for input\n",
    "    query = input(\"Ask Empath-AI\")\n",
    "\n",
    "    # Step 1: Embed the query using Gemini Pro\n",
    "    query_embedding = generate_query_embedding(query)\n",
    "\n",
    "    # Step 2: Retrieve relevant chunks from Pinecone based on query embedding\n",
    "    retrieved_chunks = retrieve_relevant_chunks(query_embedding)\n",
    "    print(\"Retrived Chunks:\",retrieved_chunks)\n",
    "\n",
    "    # Step 3: Generate an answer using Gemini Pro LLM with the retrieved chunks\n",
    "    answer = generate_answer_with_gemini(query, retrieved_chunks)\n",
    "\n",
    "    # Display the final answer to the user\n",
    "    print(\"\\nEMPATH-AI's Answer:\")\n",
    "    print(answer)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
